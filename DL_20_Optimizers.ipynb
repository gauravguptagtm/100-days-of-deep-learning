{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjqXRal+YETvFVi6dJ9Cj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravguptagtm/100-days-of-deep-learning/blob/main/DL_20_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Role of Optimizers\n",
        "\n",
        "- Basically NN is an optimization algorithm where we try to minimize the loss.\n",
        "- We use GD as optimizer to get minima.\n",
        "\n",
        "#Types\n",
        "\n",
        "- Batch GD\n",
        "- Stochastic GD\n",
        "- Mini Batch GD"
      ],
      "metadata": {
        "id": "vi1PGbHRAdkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges in GD?\n",
        "\n",
        "- Deciding learning rate\n",
        "  - Solution for this is learning rate scheduling -> We nee to predefine it so it also a problem.\n",
        "- For different weight, we can't give different learning rate.\n",
        "- Local Minima problem\n",
        "- Saddle point problem"
      ],
      "metadata": {
        "id": "TU7_XrEBBuI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponentially Weightage Moving Average\n",
        "\n",
        "- EWMA is a technique to find trends on time series data.\n",
        "- Use in time series forecasting, financial data, signal processing, Deep learning.\n",
        "- weigtage of last point is greater than starting point.\n",
        "- Overtime, weightage of any point decreases.\n",
        "\n",
        "$$\n",
        "V_t = βV_{t-1} + (1-β)θ_t\n",
        "$$\n",
        "  where 0<β<1 and V0 =0 or V0=θ0.\n",
        "\n",
        "  β decide how much you value last point, if set high, it means you wightage past points more. While set low,it means you weightage recent point more.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PFv8pr6xFh9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convex VS Non-convex Optimization\n",
        "Convex functions have a unique global minimum, making optimization easier and more reliable. Non-convex functions, on the other hand, can have multiple local minima, saddle point, high curvature making optimization more challenging.\n"
      ],
      "metadata": {
        "id": "BaCvO1DkLk8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD with Momentum\n",
        "\n",
        "Why?\n",
        "  - Works good in high curvature, get out from local minima.\n",
        "  - if slope is less, it rorks good there too.\n",
        "\n",
        "What?\n",
        "  - Suppose if we going from A to B, we speedup if suppose last 4 point send me to same direction.\n",
        "  - Same going on with momentum.\n",
        "  - It is faster than GD because of this.\n",
        "\n",
        "How?\n",
        "- It accumulate momentum by taking exponentially decaying moment of past gradient.\n",
        "\n",
        "$$\n",
        "W_{t+1} = W_t - V_t \\\\\n",
        "V_t = βV_{t-1} + ηδW_t\n",
        "$$\n",
        "β = decaying factor\n",
        "\n",
        "\n",
        "Momentum Optimizer in Deep Learning is a technique that reduces the time taken to train a model.  The path of learning in mini-batch gradient descent is zig-zag, and not straight. Thus, some time gets wasted in moving in a zig-zag direction. Momentum Optimizer in Deep Learning smooths out the zig-zag path and makes it much straighter, thus reducing the time taken to train the model."
      ],
      "metadata": {
        "id": "iJ1YjLDBJ95U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nesterov Accelerated Gradient(NAG)\n",
        "\n",
        "- It's an improvement over momentum by reducing the oscillation.\n",
        "- Formula\n",
        "  - In NAG, we take jump of mementum first, and at that place we calculate gradient and take jump according to that.\n",
        "\n",
        "  $$\n",
        "  W_{la} = W_t - βV{t-1} \\\\\n",
        "  V_t = βV{t-1} + η∇W_{la} \\\\\n",
        "  W_{t+1}  = W_t - V_t\n",
        "  $$\n",
        "\n",
        "  where la = lookahead\n",
        "  "
      ],
      "metadata": {
        "id": "xBrUNPngSHRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The acceleration of momentum can overshoot the minima at the bottom of basins or valleys. Nesterov momentum is an extension of momentum that involves calculating the decaying moving average of the gradients of projected positions in the search space rather than the actual positions themselves.\n",
        "\n",
        "This has the effect of harnessing the accelerating benefits of momentum whilst allowing the search to slow down when approaching the optima and reduce the likelihood of missing or overshooting it."
      ],
      "metadata": {
        "id": "SEVk-zypV0Rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCeX7ougAGGY"
      },
      "outputs": [],
      "source": [
        "tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01, momentum=0.9, nesterov=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01, momentum=0.9, nesterov=True\n",
        ")"
      ],
      "metadata": {
        "id": "bs_TgE6JXgFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AdaGrad\n",
        "\n",
        "- Adaptive Gradient Algorithm (Adagrad) is an algorithm for gradient-based optimization. The learning rate is adapted component-wise to the parameters by incorporating knowledge of past observations.\n",
        "- If in our input feature, they scale differently, this works good.\n",
        "- If our feature is sparse, there it works good.\n",
        "- Because of sparse feature, we have Elongated Bowl Problem.\n",
        "- We set different learning rate for different parameter. If update is big, we set learning rate small.\n",
        "- If update is small, we set learning rate big.\n",
        "\n",
        "$$\n",
        "V_t = V_{t-1} + (∇W_t)^2 \\\\\n",
        "W_{t+1} = W_t - \\frac{\\eta}{V_t+ϵ}∇W_t\n",
        "$$\n"
      ],
      "metadata": {
        "id": "chUUeKZRXrOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantages**\n",
        "- AdaGrad never converge, it can only reach nearby.\n",
        "- One of the reason, we don't use generally it in NN."
      ],
      "metadata": {
        "id": "OzNlmcowb2zV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop\n",
        "\n",
        "- Root Mean Square Propogation\n",
        "- Improvement over AdaGrad\n",
        "$$\n",
        "V_t = \\beta V_{t-1} + (1-\\beta)(∇W_t)^2 \\\\\n",
        "W_{t+1} = W_t - \\frac{\\eta}{V_t+ϵ}∇W_t\n",
        "$$\n",
        "\n",
        "- We taken here exponentially decay average.\n",
        "\n"
      ],
      "metadata": {
        "id": "mmqcTGLtcPZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam\n",
        "\n",
        "- Adaptive Moment Estimation\n",
        "- Here we use both ideas, momentum and learning rate decay.\n",
        "\n",
        "$$\n",
        "W_{t+1} = W_t - \\frac{\\eta}{V_t+ϵ}m_t\\\\\n",
        "where \\\\\n",
        "m_t = \\beta_1 m_{t-1} + (1-\\beta_1)∇W_t \\\\\n",
        "V_t = \\beta_2 V_{t-1} + (1-\\beta_2)(∇W_t)^2 \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "azQNfIwwlzV-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TCVxixtbzbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}