{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr6wHhiYkJcPLpGYwSOzxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravguptagtm/deep-learning-notebook/blob/main/DL_9(Loss_Function).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "It's a method of evaluating how well your algorithm is modelling your dataset.\n",
        "\n",
        "## Importance of Loss Function?\n",
        "\n",
        "- You can't improve what you can't measure.\n",
        "\n",
        "## Loss function in DL\n",
        "\n",
        "* Regression\n",
        "  *  MSE\n",
        "  * MAE\n",
        "  * Huber Loss\n",
        "* Classification\n",
        "  * Binary Crossentropy\n",
        "  * Categorical Crossentropy\n",
        "  * Hinge Loss\n",
        "* Autoencoders\n",
        "  * Kl Divergence\n",
        "* GAN\n",
        "  * Discriminator Loss\n",
        "  * Minmax GAN Loss\n",
        "* Object Detection\n",
        "  * Focal loss\n",
        "* Embedding\n",
        "  * Triplet Loss\n",
        "\n"
      ],
      "metadata": {
        "id": "8_wH91QpEv8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function VS Cost function\n",
        "\n",
        "- When calculate for single training example - loss function\n",
        "- When calculate for whole batch - cost function\n",
        "\n"
      ],
      "metadata": {
        "id": "N1AxwqM9HsVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error (MSE) / L2 loss\n",
        "\n",
        "    \n",
        "\n",
        "$$\n",
        "L = (y_i-ŷ_i)^2\n",
        "$$\n",
        "\n",
        "Why we do squared? if error is less, they are punish less. While point having more distance punish more.\n",
        "\n",
        "- It's easy to interpret.\n",
        "- Always differentiable(GD).\n",
        "- One local minima.\n",
        "\n",
        "Disadvantages:\n",
        "- Error Unit(squared)\n",
        "- Not robust to outliers.\n",
        "- Works when you use linear activation in last neuron. (Regression)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1YVEVfq5IUTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Absolute Error (MAE) / L1 loss\n",
        "\n",
        "$$\n",
        "L = |y_i - ŷ_i|\n",
        "$$\n",
        "\n",
        "- Easy to understand\n",
        "- Unit - same - y\n",
        "- Robust to Outliers.\n",
        "\n",
        "Disadvantages:\n",
        "- Not always differentiable\n"
      ],
      "metadata": {
        "id": "CsstejV2ynyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huber Loss\n",
        "- It is a combination of MAE and MSE.\n",
        "- Lies b/w the MAE and MSE.\n",
        "- Works better with more outliers."
      ],
      "metadata": {
        "id": "QvY2BLL20sEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Cross Entropy\n",
        "\n",
        "- Used in classification\n",
        "- When having two classes.\n",
        "$$\n",
        "    L = -ylog(ŷ) - (1-y)log(1-ŷ)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "f8lTixX23Xs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "- Used in softmax regression\n",
        "- Multi-class Classification\n",
        "- activation function in output layer is softmax.\n",
        "\n",
        "$$\n",
        "    L = -y1log(ŷ1) - y2log(ŷ2) - ...\n",
        "$$\n",
        "\n",
        "- We do Onehot encoding before"
      ],
      "metadata": {
        "id": "Ii4zYQU947MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "- Here we integer encode the y.\n",
        "- Everything same as upper one\n",
        "- Relativily fast"
      ],
      "metadata": {
        "id": "dDWze5F7Qyjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_gL2ZbHEo4x"
      },
      "outputs": [],
      "source": []
    }
  ]
}